{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Evaluation\n",
    "\n",
    "Now that we have a trained model, it's time to evaluate its performance on the test set. This will tell us how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Import Libraries and Reload Model\n",
    "\n",
    "First, let's import the necessary libraries. We need to redefine our `SimpleCNN` class so we can load the saved weights into it. We'll also load the test dataset and apply the same transformations as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets torch torchvision matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Redefine the model class (must be identical to the one used for training)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 16 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Load the trained model weights\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('deepfake_detector_model.pth'))\n",
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Prepare the Test Data\n",
    "\n",
    "We'll load the test split of the dataset and apply the same transformations we used in training to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def apply_transforms(examples):\n",
    "    examples['pixel_values'] = [data_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "# Load the dataset and apply transformations\n",
    "test_dataset_raw = load_dataset(\"saakshigupta/deepfake-detection-dataset-v3\", split='test')\n",
    "test_dataset_transformed = test_dataset_raw.map(apply_transforms, batched=True)\n",
    "test_dataset_transformed.set_format('torch', columns=['pixel_values', 'label'])\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_loader = DataLoader(test_dataset_transformed, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Evaluate the Model and Calculate Accuracy\n",
    "\n",
    "We'll loop through the test data, get the model's predictions, and compare them to the true labels to calculate the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# No need to calculate gradients during evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['pixel_values']\n",
    "        labels = batch['label'].float().view(-1, 1)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float() # Get binary predictions (0 or 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Visualize Predictions\n",
    "\n",
    "Let's look at a few random images from the test set and see what the model predicted versus the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, title):\n",
    "    img = img / 2 + 0.5 # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get a random batch of test data\n",
    "random.seed(42)\n",
    "indices = random.sample(range(len(test_dataset_transformed)), k=4)\n",
    "sample_data = [test_dataset_transformed[i] for i in indices]\n",
    "raw_images = [test_dataset_raw[i]['image'] for i in indices]\n",
    "sample_loader = DataLoader(sample_data, batch_size=4)\n",
    "\n",
    "# Get predictions\n",
    "dataiter = iter(sample_loader)\n",
    "batch = next(dataiter)\n",
    "inputs = batch['pixel_values']\n",
    "labels = batch['label']\n",
    "outputs = model(inputs)\n",
    "predicted = (outputs > 0.5).squeeze()\n",
    "\n",
    "# Plot the images with their predicted labels\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "label_map = {1: 'Real', 0: 'Fake'}\n",
    "\n",
    "for i in range(4):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(raw_images[i])\n",
    "    true_label = label_map[labels[i].item()]\n",
    "    pred_label = label_map[predicted[i].item()]\n",
    "    ax.set_title(f'True: {true_label}\\nPred: {pred_label}', color=('green' if true_label == pred_label else 'red'))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "\n",
    "Congratulations! We have successfully built, trained, and evaluated a deepfake detection model.\n",
    "\n",
    "We started by exploring the data, then trained a CNN model on it, and finally evaluated its performance on a held-out test set. \n",
    "\n",
    "### Future Improvements:\n",
    "- **Use a more complex model**: A simple CNN is a good start, but more advanced architectures like ResNet or EfficientNet could provide better accuracy.\n",
    "- **Train on more data**: We only used a subset of the data for quick training. Training on the full dataset for more epochs would likely improve performance.\n",
    "- **Data Augmentation**: Applying random transformations (like rotations, flips, and color jitter) to the training data can help the model generalize better.\n",
    "- **Video Analysis**: This model only processes images. A more advanced system could analyze video frames sequentially, possibly using an LSTM or Transformer model to capture temporal inconsistencies common in deepfakes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
